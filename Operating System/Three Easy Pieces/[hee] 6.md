### 1. 물리 메모리 크기의 극복 : 정책
- 빈 메모리 공간이 거의 없는 경우 운영체제는 메모리 압박(memory pressure)을 해소하기 위해 다른 페이지들을 강제적으로 페이징 아웃(paging out)하여 활발히 사용 중인 페이지들을 위한 공간을 확보함
- 내보낼(evict) 페이지(또는 페이지들) 선택은 운영체제의 교체 정책(replacement policy) 안에 집약되어 있음


<br>

#### 1.1. 캐시 관리
- 시스템의 전체 페이지들 중 일부분만이 메인 메모리에 유지된다는 것을 가정하면, 메인 메모리는 시스템의 가상 메모리 페이지를 가져다 놓기 위한 캐시로 생각될 수 있음
- 이 캐시를 위한 교체 정책의 목표는 캐시 미스의 횟수를 최소화하는 것
  - 즉, 디스크로부터 페이지를 가져오는 횟수를 최소로 만드는 것
- 한편으로 캐시 히트 횟수를 최대로 하는 것도 목표라고 할 수 있음
  - 즉, 접근된 페이지가 메모리에 이미 존재하는 횟수를 최대로 하는 것

<br>

- 캐시 히트와 미스의 횟수를 안다면 프로그램의 평균 메모리 접근 시간(average memory access time, AMAT)을 계산할 수 있음
- 히트와 미스의 정보를 안다면 프로그램의 AMAT는 다음과 같은 식으로 계산할 수 있음
  ```
   AMAT = (PHit · TM) + (PMiss · TD)
  ```
    - TM은 메모리 접근 비용
    - TD는 디스크 접근 비용
    - PHit는 캐시에서 데이터 항목을 찾을 확률(히트)
    - PMiss는 캐시에서 데이터를 못찾을 확률(미스)
    - PHit와 PMiss는 각각 0.0에서 1.0 사이의 값을 가지며 PMiss + PHit = 1.0을 만족함

<br>

- 현대 시스템에서는 디스크 접근 비용이 너무 크기 때문에 아주 작은 미스가 발생하더라도 전체적인 AMAT에 큰 영향을 주게됨
- 컴퓨터가 디스크 속도 수준으로 느리게 실행되는 것을 방지하기 위해서는 당연히 미스를 최대한 줄여야 하며, 이를 위해서는 좋은 정책을 잘 만드는 것



<br>

#### 1.2. 최적 교체 정책
- 교체 정책의 동작 방식을 잘 이해하기 위해서 최적 교체 정책(The Optimal Replacement Policy)과 비교하는 것이 좋음
- 최적 교체 정책은 미스를 최소화함
- 가장 나중에 접근될 페이지를 교체하는 것이 최적이며, 가장 적은 횟수의 미스를 발생시킨다는 것을 증명하였음
- 이 정책은 간단하지만 구현하기는 어려운 정책

<br>

- 캐시는 처음에 비어 있는 상태로 시작하기 때문에 처음 접근은 당연히 미스
- 이러한 종류의 미스는 때로는 최초 시작 미스(cold-start miss) 또는 강제 미스(compulsory miss)라고 함

<br>

- 미래는 일반적으로 미리 알 수 없기 때문에 범용 운영체제에서는 최적 기법의 구현은 불가능
- 내보낼 페이지를 결정하기 위한 실제적이고 배포가 가능한 정책을 만들기 위해 다른 방법을 찾는것에 집중할 것
- 최적 기법은 비교 기준으로만 사용될 것이며, 이를 통해 '정답'에 얼마나 가까운지 알 수 있음




<br>

#### 1.3. 간단한 정책 : FIFO
- 초기의 많은 시스템들은 최적의 방법에 도달하려는 복잡한 시도를 피하고 대신 매우 간단한 교체 정책을 채용
- 일부 시스템에서는 FIFO( 선입선출) 교체 방식을 사용함
  - 이 방법은 시스템에 페이지가 들어오면 큐에 삽입되고, 교체를 해야 할 경우 큐의 테일에 있는 페이지가(먼저 들어온 페이지) 내보내짐
  - 매우 구현하기 쉽다는 장점
  - 최적의 경우와 비교하면 FIFO는 눈에 띄게 성능이 안 좋으며, FIFO는 블럭들의 중요도를 판단할 수가 없음 




<br>

#### 1.4. 또 다른 간단한 정책 : 무작위 선택
- 이 방식은 메모리 압박이 있을 때 페이지를 무작위로 선택하여 교체함
- 무작위 선택 방식은 FIFO와 유사한 성질을 가지고 있음
- 구현하기 쉽지만 내보낼 블럭을 제대로 선택하려는 노력을 하지 않음
- 무작위 선택 정책은 선택할 때 얼마나 운이 좋은지(또는 운이 나쁜지)에 전적으로 의존




<br>

#### 1.5. 과거 정보의 사용 : LRU
- FIFO 또는 무작위 선택 방식처럼 단순한 정책들은 중요한 페이지들을 혹은 바로 다시 참조하게 될 것들을 내보낼 수 있다는 비슷한 문제를 겪게됨
- FIFO와 무작위 선택 방식 그리고 그와 유사한 정책들은 최적 기법의 성능을 따라갈 수가 없기 때문에 좀 더 정교한 방식이 필요

<br>

- 페이지 교체 정책이 활용할 수 있는 과거 정보 중 하나는 빈도수(frequency)
- 좀 더 자주 사용되는 페이지의 특징은 접근의 최근성(recency)
- 이러한 류의 정책은 지역성의 원칙(principle of locality)이라고 부르는 특성에 기반을 둠
- 이 원칙은 프로그램의 행동 양식을 관찰하여 얻은 결과
- 프로그램들은 특정 코드들과 자료 구조를 상당히 빈번하게 접근하는 경향이 있다는 것
  - 코드의 예로는 반복문 코드, 자료 구조로는 그 반복문에 의해 접근되는 배열
- 과거의 현상을 보고 어떤 페이지들이 중요한지 판단하고 내보낼 페이지를 선택할 때 중요한 페이지들은 메모리에 보존하는 것

<br>

- 그리하여 과거 이력에 기반한 교체 알고리즘 부류가 탄생하게 됨
- LeastFrequently-Used(LFU) 정책은 가장 적은 빈도로 사용된 페이지를 교체함
- LeastRecently-Used(LRU) 정책은 가장 오래 전에 사용하였던 페이지를 교체함




<br>

#### 1.6. 워크로드에 따른 성능 비교
- 워크로드에 지역성이 없다면 어느 정책을 사용하든 상관이 없음
  - LRU와 FIFO 그리고 무작위 선택 정책은 모두 동일한 성능을 보이며, 히트율은 정확히 캐시의 크기에 의해서 결정됨
- 캐시가 충분히 커서 모든 워크로드를 다 포함할 수 있다면 역시 어느 정책을 사용하든지 상관 없음
  - 참조되는 모든 블럭들이 캐시에 들어갈 수 있으면 모든 정책들은(무작위 선택마저도) 히트율이 100%에 도달함
- 최적 기법이 구현 가능한 기타 정책들보다 눈에 띄게 더 좋은 성능을 보임
  - 미래를 알 수 있다면 교체 작업을 월등히 잘할 수 있음

<br>

- 80대 20 워크로드
  - 20%의 페이지들에서(인기 있는 페이지) 80%의 참조가 발생하고 나머지 80%의 페이지들에 대해서 20%의 참조만(비인기 페이지) 발생
  - 랜덤과 FIFO 정책이 상당히 좋은 성능을 보이지만, 인기있는 페이지들을 캐시에 더 오래두는 경향이 있는 LRU가 더 좋은 성능을 보임
    - 인기있는 페이지들이 과거에 빈번하게 참조되었기 때문에 그 페이지들은 가까운 미래에 다시 참조되는 경향이 있기 때문
    - 최적 기법은 여전히 더 좋은 성능을 보이고 있으며, 이는 LRU의 과거 정보가 완벽하지는 않다는 것을 보여줌

<br>

- 순차 반복 워크로드
  - 50개의 페이지들을 순차적으로 참조
  - LRU와 FIFO 정책에서 가장 안좋은 성능을 보임
  - 무작위 선택 정책은 최적의 경우에 못 미치기는 하지만 눈에 띄게 좋은 성능을 보임
    - 이상한 코너 케이스가 발생하지 않는다는 좋은 특성을 가짐





<br>

#### 1.7. 과거 이력 기반 알고리즘의 구현
- 중요한 페이지들을 내보낼 수도 있는 단순한 FIFO와 무작위 선택 정책들보다 LRU와 같은 알고리즘이 더 좋은 성능을 보임
- 하지만 과거 정보에 기반을 둔 정책은 새로운 문제점이 있음 -> 어떻게 구현할지
- LRU에서는 어떤 페이지가 가장 최근에 또는 가장 오래 전에 사용되었는지를 관리하기 위해서 모든 메모리 참조 정보를 기록해야 함
- 세심한 주의 없이 정보를 기록하면 성능이 크게 떨어질 수 있음




<br>

#### 1.8. LRU 정책 근사하기
- LRU 정책에 가까운 구현이 가능하며, 연산량이라는 관점에서 볼 때 LRU를 '근사'하는 식으로 만들면 구현이 훨씬 쉬워짐
- 실제로 현대의 많은 시스템이 이런 방식을 택하고 있음
- 이 개념에는 use bit(때로는 reference bit라고도 불림)라고 하는 약간의 하드웨어 지원이 필요함
- 이 기법은 페이징을 최초로 적용한 Atlas one-level store 시스템에 처음으로 구현되었음

<br>

- 각 페이지마다 하나의 use bit가 있으며, 이 use bit는 메모리 어딘가에 존재함(구현에따라 프로세스마다 가지고 있는 페이지 테이블에 있을 수도 있고 또는 어딘가에 배열의 형태로 존재할 수도 있음)
- 페이지가 참조될 때마다(읽히거나 기록되면) 하드웨어에 의해서 use bit가 1로 설정됨
- 하드웨어는 이 비트를 절대로 지우지 않음(0으로 설정하지 않음)
- 0으로 바꾸는 것은 운영체제의 몫이며, 운영체제는 LRU에 가깝게 구현하기 위해서 use bit를 활용함
- 간단한 활용법이 시계 알고리즘(clock algorithm)에서 제시됨



<br>

#### 1.9. 갱신된 페이지(Dirty Page)의 고려
- 운영체제가 교체 대상을 선택할때 메모리에 탑재된 이후에 변경되었는지를 추가적으로 고려하는 것
- 필요한 이유
  - 만약에 어떤 페이지가 변경(modiied)되어 더티(dirty) 상태가 되었다면, 그 페이지를 내보내기 위해서는 디스크에 변경 내용을 기록해야 하기 때문에 비싼 비용을 지불해야함
- 이와 같은 동작을 지원하기 위해서 하드웨어는 modiied bit(더티 비트라고도 불림)를 포함해야 함
  - 페이지가 변경될 때마다 이 비트가 1로 설정되므로 페이지 교체 알고리즘에서 이를 고려하여 교체 대상을 선택함




<br>

#### 1.10. 다른 VM 정책들
- 페이지 교체 정책만이 VM 시스템이 채택하는 유일한 정책은 아님(그 중에 가장 중요한것이기는 함)
- 예를 들어, 운영체제는 언제 페이지를 메모리로 불러들일지 결정해야함
  - 이 정책은 운영체제에게 몇 가지 다른 옵션을 제공하며, 이 정책은 Denning이 불렀던 것처럼 때로는 페이지 선택(page selection) 정책이라고 불림

<br>

- 운영체제는 대부분의 페이지를 읽어 들일 때 요구 페이징(demand paging) 정책을 사용함
  - 이 정책은 말 그대로 페이지가 실제로 접근될 때 운영체제가 해당 페이지를 메모리로 읽어 들임
  - 운영체제는 어떤 페이지가 곧 사용될 것이라는 것을 대략 예상할 수 있기 때문에 미리 메모리로 읽어 들일 수도 있음
  - 이와같은 동작을 선반입(prefetching)이라고 하며 성공할 확률이 충분히 높을 때에만 해야함

<br>

- 또 다른 정책은 운영체제가 변경된 페이지를 디스크에 반영하는데 관련된 방식
  - 한 번에 한 페이지씩 디스크에 쓸 수 있지만, 많은 시스템은 기록해야 할 페이지들을 메모리에 모은 후 한 번에(더 효율적으로) 디스크에 기록
  - 이와 같은 동작을 클러스터링(clustering) 또는 단순하게 쓰기 모으기(grouping of writes)라고 부르며 효과적인 동작 방식





<br>

#### 1.11. 쓰래싱(Thrashing)
- 메모리 사용 요구가 감당할 수 없을 만큼 많고 실행 중인 프로세스가 요구하는 메모리가 가용 물리 메모리 크기를 초과하는 경우에 운영체제는 끊임없이 페이징을 할 수밖에 없고, 이와 같은 상황을 쓰래싱(thrashing)이라고 부름
- 몇몇 초기 운영체제들은 쓰래싱이 발생했을 때, 이의 발견과 해결을 위한 상당히 정교한 기법들을 가지고 있었음
- 워킹 셋(working set)이란 프로세스가 실행중에 일정 시간 동안 사용하는 페이지들의 집합
- 일반적으로 진입 제어(admission control)라고 알려져 있는 이 방법은 많은 일들을 엉성하게 하는 것보다는 더 적은 일을 제대로 하는 것이 나을 때가 있다고 말함
- 일부 최신 시스템들은 메모리 과부하에 대하여 좀 더 과감한 조치를 취하기도 함
- 일부 버전의 Linux는 메모리 요구가 초과되면 메모리 부족 킬러(out-ofmemory killer)를 실행시킴







<br>
<br>

### 2. VAX/VMS 가상 메모리 시스템
#### 2.1. 배경
- VAX-11 미니컴퓨터의 구조는 1970년대 말에 Digital Equipment Corporation(DEC)에 의해서 소개되었음
- 이 구조는 VAX-11/780과 그 보다는 낮은 성능의 VAX-11/750을 포함한 컴퓨터에 구현되었음
- VAX/VMS(또는 단순하게 VMS)라고 알려진 운영체제의 주요한 아키텍트는 Dave Cutler로서 이후에는 Microsoft Windows NT 개발을 이끌었음
- VMS 운영체제는 다양한 종류의 시스템들에서 동작하는(그것도 잘 동작하는) 기법들과 정책들을 필요로 했음
- VMS는 컴퓨터의 구조적 결함을 소프트웨어로 보완한 훌륭한 사례



<br>

#### 2.2. 메모리 관리 하드웨어
- VAX-11은 프로세스마다 512바이트 페이지 단위로 나누어진 32비트 가상 주소 공간을 제공함
- 가상 주소는 23비트 VPN과 9비트 오프셋으로 구성되어 있음
- VPN의 상위 두 비트는 페이지가 속한 세그멘트를 나타내기 위해서 사용됨
- 이 시스템은 페이징과 세그멘테이션의 하이브리드 구조를 갖고 있음

<br>

- 주소 공간의 하위 절반은 '프로세스 공간'으로 알려져 있으며 각 프로세스마다 다르게 할당됨
- 프로세스 공간의 첫 번째 절반(P0으로 알려져 있음)에 사용자 프로그램과 힙(heap)이 존재
  - 힙은 주소가 큰 쪽으로 증가함
- 프로세스 공간의 두 번째, 즉 큰쪽 절반은(P1) 주소가 작은 방향으로 증가하는 스택(stack)이 존재
- 주소 공간의 상위 절반은 그 중 반만 사용되며 시스템 공간(S)으로 불림
  - 운영체제의 보호된 코드와 데이터가 이곳에 존재하며, 이 방식으로 여러 프로세스가 운영체제를 공유함

<br>

- VMS 설계자들의 주요 고민 중 하나는 믿을 수 없을 정도로 작은 VAX 하드웨어의 페이지 크기(512바이트)
- 역사적인 이유로 페이지 크기가 결정되었지만 선형 페이지 테이블의 크기가 지나치게 커진다는 것이 근본적인 문제였음
- VMS 설계자들의 첫 목표중에 하나는 VMS가 페이지 테이블 저장을 위해 메모리를 소진하는 것을 막는 것
- 이 시스템은 페이지 테이블로 인한 메모리 압박의 정도를 경감시키기 위한 두 가지 방법을 사용하였음
  - 첫째, VAX-11은 사용자 주소 공간을 두 개의 세그멘트로 나누어 프로세스마다(P0과 P1) 각 영역을 위한 페이지 테이블을 가지도록 하였음
  - 둘째로, 운영체제는 사용자 페이지 테이블들을(P0과 P1, 즉 프로세스마다 두 개) 커널의 가상 메모리에 배치하여 메모리 압박을 더 줄일 수 있었음





<br>

#### 2.3. 실제 주소 공간
- 코드 세그멘트는 절대로 페이지 0에서 시작하지 않으며, 대신 이 페이지는 접근 불가능 페이지로 마킹되어 있으며 널-포인터(null-pointer) 접근을 검출할 수 있게함
- 주소 공간 설계 시 한 가지 고려해야 할 사항은 효과적인 디버깅 지원 여부
  - 접근 불가능한 페이지 0은 그런 지원의 한 형태
- 좀 더 중요한 사실은 커널의 가상 주소 공간이(즉, 커널의 자료 구조와 코드) 사용자 주소 공간의 일부라는 것
  - 문맥 교환이 발생하면 운영체제는 P0과 P1 레지스터를 다음 실행될 프로세스의 페이지 테이블을 가리키도록 변경함
  - 하지만, S 베이스와 바운드 레지스터는 변경하지 않기 때문에 결과적으로 '동일한' 커널 구조들이 각 사용자 주소 공간에 매핑됨

<br>

- 몇 가지 이유로 커널은 여러 주소 공간들로 매핑됨
- 그러한 구조를 택하면 커널이 동작이 쉬워짐

<br>

- 주소 공간에 관한 마지막 이슈는 보호와 관련 있음
- 운영체제는 응용 프로그램이 운영체제의 데이터나 코드를 읽거나 쓰는 것을 분명히 원하지 않으며, 운영체제의 자료를 보호하기 위해서는 하드웨어가 페이지 별로 보호 수준을 다르게 설정할 수 있어야함
- 이를 위해 VAX는 페이지 테이블의 protection bit(protection bits)에 보호 수준을 지정
- 특정 페이지를 접근하기 위해서 필요한 CPU의 권한 수준이 기록됨
- 시스템 데이터와 코드는 사용자의 데이터와 코드보다 더 높은 보호 수준으로 지정됨






<br>

#### 2.4. 페이지 교체
- VAX의 페이지 테이블 항목(PTE)은 아래와 같은 비트들을 가지고 있음
  - 유효(valid) 비트
  - 보호 필드(protection ield, 4비트)
  - 변경(modify 또는 더티(dirty)) 비트
  - 운영체제가 사용하기 위해 예약해 놓은 필드(5비트)
  - 물리 메모리 페이지의 위치를 저장하기 위한 물리 프레임 번호(PFN)

<br>

- 문제
  - VMS 교체 알고리즘은 어떤 페이지가 자주 사용 중인지를 하드웨어 지원 없이 판단하여야 함
  - 개발자들은 메모리를 너무 많이 사용하는 프로그램을 의미하는 메모리 호그(memory hog)에 대해서도 고민하였음

<br>

- 해결책
  - 세그멘트된 FIFO 교체 정책
    - 각 프로세스는 상주 집합 크기(resident set size, RSS)라고 불리는 메모리에 유지할 수 있는 최대 페이지 개수를 지정 받음
    - 각 페이지들은 FIFO 리스트에 보관되며, 페이지 개수가 RSS보다 커지면 '제일 먼저 들어왔던' 페이지가 쫓겨남
    - FIFO는 하드웨어의 지원이 필요 없기 때문에 구현이 간단함
    - 순수한 FIFO 성능은 앞서 보았던 것처럼 그리 좋지는 않기 때문에, FIFO의 성능을 개선하기 위해서 VMS는 전역 클린-페이지 프리 리스트(global clean-page free list)와 더티페이지 리스트(dirty-page list)라고 하는 두 개의 second-chance list를 도입
    - 이 리스트는 전역 자료 구조이며, Second-chance list는 메모리에서 제거되기 전에 페이지가 보관되는 리스트
    - 전역 second-chance list의 크기가 클수록 세그멘트된 FIFO 알고리즘은 LRU와 유사하게 동작함

  - 페이지 클러스터링
    - 스왑할 때 I/O의 효율을 개선하기 위해 VMS는 몇 가지 최적화 기법을 도입
    - 그 중에서 가장 중요한 것은 클러스터링(clustering)이며, 클러스터링 기법을 써서 VMS는 전역 더티 리스트에 있는 페이지들을 작업 묶음을 만들어서 한 번에 디스크로 보냄(그렇게 하여 페이지를 클린 상태로 만듬) 
    - 쓰기 횟수는 줄이고 한 번에 쓰는 양은 늘려서 성능을 향상시키기 때문에 클러스터링은 대부분의 현대 시스템에서 사용됨






<br>

#### 2.5. 그 외의 VM 기법들
- VMS는 이제는 표준화가 된 기법 두 가지를 더 가지고 있음
- 요청 시 0으로 채우기(demand zeroing)
  - VMS(그리고 대부분의 현대의 시스템들)가 사용하는 기법의 한 형태는 페이지들을 demand zeroing
  - 프로세스가 해당 페이지를 사용하지 않는다면 너무 많은 비용을 지불하는 셈
  - Demand zeroing의 경우 페이지가 주소 공간에 추가되는 시점에는 거의 하는 일이 없음
  - 페이지 테이블에 접근 불가능 페이지라고 표기하고 항목을 추가함
  - 프로세스가 추가된 페이지를 읽거나 쓸 때 운영체제로 트랩이 발생함
  - 트랩을 처리하면서 운영체제는 demand zeroing 할 페이지라는 것을 알게 됨(일반적으로는 페이지 테이블 항목의 '운영체제를 위해 예약된' 부분에 일정 비트로 표기되어 있음)
  - 이 시점에서 운영체제는 물리 페이지를 0으로 채우고 프로세스의 주소 공간으로 매핑하는 등의 필요한 작업을 함 
  - 프로세스가 해당 페이지를 전혀 접근하지 않는다면 이 모든 작업을 피할 수 있으며, 이것이 바로 demand zeroing의 장점

- 쓰기-시-복사(copy-on-write)
  - 운영체제가 한 주소 공간에서 다른 공간으로 페이지를 복사할 필요가 있을 때, 복사를 하지 않고 해당 페이지를 대상 주소 공간으로 매핑하고 해당 페이지의 페이지 테이블 엔트리를 양쪽주소 공간에서 읽기 전용으로 표시함
  - 만약 양쪽 주소 공간이 페이지를 읽기만 한다면 더 이상의 조치는 필요 없게 되며, 운영체제는 실제로 데이터 이동 없이 빠른 복사를 할 수 있게 됨
  - 두 주소 공간 중에 하나가 페이지 쓰기를 시도한다면, 운영체제로 트랩을 발생
  - 운영체제는 그때 해당 페이지가 COW 페이지라는 것을 파악함
  - 그런 후에 새로운 페이지를 (게으르게) 할당하고, 데이터로 채우고, 이 새로운 페이지 폴트를 일으킨 페이지의 주소 공간에 매핑함
  - 공유 라이브러리들을 여러 프로세스들의 주소 공간에 copy-on-write로 매핑하여 메모리 공간을 절약할 수 있음
  - Unix 시스템에서는 fork()와 exec()의 시맨틱 때문에 COW는 훨씬 더 중요함
    - copy-on-write fork()를 수행하게 되면 운영체제는 상당한 불필요한 복사를 피할 수 있으며, 성능을 개선하면서 정확한 시맨틱을 유지할 수 있음

